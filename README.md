# Multimodal-LLM
<h2><b><i>Earning call dataset</i></b></h2>
MAEC, a large-scale multi-modal, text-audio paired dataset derived from S&amp;P 1500 companies' earnings calls. It provides a framework for processing similar financial data in the future, offering a valuable resource for research in financial risk prediction and analysis.

# About Multi-Modal LLM
A Model that can process and understand information from multiple modalities, such as text, audio, images, and video. Unlike traditional LLMs that focus solely on text, multimodal LLMs can leverage the strengths of different modalities to gain a richer understanding of the world.
# Capabilties
<blockquote> * Access to multiple modalities can improve the model's ability to generalize to unseen data. This is because the model learns not only from the content of each modality but also from the relationships between them.
* can combine information from various modalities to create a more comprehensive representation of the data. This allows them to perform tasks that would be challenging for models restricted to a single modality. </blockquote>
